{
    "experiment":{
        "experiment_id": "basev1",
        "experiment_name": "-base",
        "experiment_log_prefix": "",
        "wandb_project": "Behavioral-Learning",
        "wandb_group": "self-play-2D",
        "wandb_entity": "iu_mas",
        "wandb_notes": "Base config, based on last version of configurations for train_selfplay_baselines.py",

        "env": "SelfPlay1v1-Pred_Prey-v0",
        "num_workers": 1,
        "num_rounds": 50,
        "population_size": 1,
        "parallel_alternate_training": false,
        "seed_value": 3,
        "framework": "stable_baselines3",

        "hierarchy":"2D:evorobotpy2:predprey:1v1",

        "agents_order": {"0":"pred", "1":"prey"}
    },

    "agent0":{
        "id": 0,
        "name": "pred",
        "env_class": "SelfPlayPredEnv",
        "opponent_name":"prey",
        "obs": "full",
        "act": "vel",

        "num_eval_episodes": 5,  
        "num_heatmap_eval_episodes": 5,  
        "num_timesteps": 25e3,
        "eval_freq": "$num_timesteps",
        "save_freq": "$num_timesteps",
        "final_save_freq": 3,
        "heatmap_log_freq":3,
        "aggregate_eval_matrix":false,
        "eval_matrix_testing_freq":3,

        
        "__comment1__": "It is possible to use winrate, random, latest, latest-set, highest, highest-set, lowest, lowest-set",
        "eval_metric": "winrate",
        "eval_opponent_selection": "random",
        "opponent_selection": "random",
        "training_agent_selection": "latest",

        "num_sampled_opponent_per_round": 50,
        "sample_after_rollout": true,

        "rl_algorithm": "PPO",
        "policy": "MlpPolicy",
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "batch_size":64,
        "gamma":0.99,
        "lr": 3e-4,
        "n_epochs": 10
    },

    "agent1":{
        "id": 0,
        "name": "prey",
        "env_class": "SelfPlayPreyEnv",
        "opponent_name":"pred",
        "obs": "full",
        "act": "vel",

        "num_eval_episodes": 5,
        "num_heatmap_eval_episodes": 5,
        "num_timesteps": 25e3,
        "eval_freq": "$num_timesteps",
        "save_freq": "$num_timesteps",
        "final_save_freq": 3,
        "heatmap_log_freq":3,
        "aggregate_eval_matrix":false,
        "eval_matrix_testing_freq":3,

        
        "__comment1__": "It is possible to use winrate, random, latest, latest-set, highest, highest-set, lowest, lowest-set",
        "eval_metric": "winrate",
        "eval_opponent_selection": "random",
        "opponent_selection": "random",
        "training_agent_selection": "latest",

        "num_sampled_opponent_per_round": 50,
        "sample_after_rollout": true,

        "rl_algorithm": "PPO",
        "policy": "MlpPolicy",
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "batch_size":64,
        "gamma":0.99,
        "lr": 3e-4,
        "n_epochs": 10
    },

    "evaluation":{
    },

    "testing":{
        "__comment__": "TODO: Think about make it more general",
        "__comment1__": "Here it is the names of the agents not the keys",
        "pred":{
            "path": "?",
            "gens": 0,
            "__comment1__": ["gen", "all", "random", "limit_s", "limit", "limit_e", "round"],
            "mode": "gen",
            "freq": 1
        },
        "prey":{
            "path": "?",
            "gens": 1,
            "__comment1__": ["gen", "all", "random", "limit_s", "limit", "limit_e", "round"],
            "mode": "gen",
            "freq": 1
        }
    }
}