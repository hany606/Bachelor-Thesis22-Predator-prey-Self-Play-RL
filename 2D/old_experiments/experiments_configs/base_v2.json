{
    "experiment":{
        "experiment_id": "basev2",
        "experiment_name": "-base-v2",
        "experiment_log_prefix": "",
        "wandb_project": "Behavioral-Learning",
        "wandb_group": "self-play-2D",
        "wandb_entity": "iu_mas",
        "wandb_notes": "Basev2 config, changed parallel_alternate_training, num_sampled_opponent, turned off sampling after rollout",

        "env": "SelfPlay1v1-Pred_Prey-v0",
        "num_workers": 1,
        "num_rounds": 50,
        "population_size": 1,
        "parallel_alternate_training": true,
        "seed_value": 3,
        "framework": "stable_baselines3",

        "hierarchy":"2D:evorobotpy2:predprey:1v1",

        "agents_order": {"0":"pred", "1":"prey"}
    },

    "agent0":{
        "id": 0,
        "name": "pred",
        "env_class": "SelfPlayPredEnv",
        "opponent_name":"prey",
        "obs": "full",
        "act": "vel",

        "num_eval_episodes": 5,  
        "num_heatmap_eval_episodes": 5,  
        "num_timesteps": 25e3,
        "eval_freq": "$num_timesteps",
        "save_freq": "$num_timesteps",
        "final_save_freq": 3,
        "heatmap_log_freq":3,

        
        "__comment1__": "It is possible to use winrate, random, latest, latest-set, highest, highest-set, lowest, lowest-set",
        "eval_metric": "winrate",
        "eval_opponent_selection": "random",
        "opponent_selection": "random",
        "training_agent_selection": "latest",

        "num_sampled_opponent_per_round": 25,
        "sample_after_rollout": false,

        "rl_algorithm": "PPO",
        "policy": "MlpPolicy",
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "batch_size":64,
        "gamma":0.99,
        "lr": 3e-4,
        "n_epochs": 10
    },

    "agent1":{
        "id": 0,
        "name": "prey",
        "env_class": "SelfPlayPreyEnv",
        "opponent_name":"pred",
        "obs": "full",
        "act": "vel",

        "num_eval_episodes": 5,
        "num_heatmap_eval_episodes": 5,
        "num_timesteps": 25e3,
        "eval_freq": "$num_timesteps",
        "save_freq": "$num_timesteps",
        "final_save_freq": 3,
        "heatmap_log_freq":3,

        
        "__comment1__": "It is possible to use winrate, random, latest, latest-set, highest, highest-set, lowest, lowest-set",
        "eval_metric": "winrate",
        "eval_opponent_selection": "random",
        "opponent_selection": "random",
        "training_agent_selection": "latest",

        "num_sampled_opponent_per_round": 25,
        "sample_after_rollout": false,

        "rl_algorithm": "PPO",
        "policy": "MlpPolicy",
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "batch_size":64,
        "gamma":0.99,
        "lr": 3e-4,
        "n_epochs": 10
    },

    "evaluation":{
    }
}