{
    "experiment":{
        "experiment_id": "basev3",
        "experiment_name": "-base3",
        "experiment_log_prefix": "",
        "wandb_project": "Behavioral-Learning",
        "wandb_group": "self-play-2D-PettingZoo",
        "wandb_entity": "iu_mas",
        "wandb_notes": "(sample-after-rollout) Base3 config, based on base.json with modifications",

        "env": "SelfPlay1v1-PZPred_Prey",
        "num_workers": 1,
        "num_rounds": 50,
        "population_size": 1,
        "parallel_alternate_training": false,
        "seed_value": 10,
        "framework": "stable_baselines3",

        "hierarchy":"2D:pettingzoo:predprey:1v1",
        "log_env_dir_name":"PettingZoo",

        "agents_order": {"0":"pred", "1":"prey"}
    },
    
    "shared":{
        "sample_after_rollout": false,
        "sample_after_reset": true,
        "randomly_reseed_sampling": true,
        "num_sampled_opponent_per_round": 1,
        "eval_metric": "lastreward"
    },

    "agent0":{
        "id": 0,
        "name": "pred",
        "env_class": "SelfPlayPZPredEnv",
        "opponent_name":"prey",
        "obs": "full",
        "act": "vel",

        "num_eval_episodes": 5,  
        "num_heatmap_eval_episodes": 5,  
        
        "num_timesteps": 25e3,

        "eval_freq": 0,
        "save_freq": 0,
        "final_save_freq": 5,

        "eval_matrix_testing_freq":3,

        "aggregate_eval_matrix":false,
        "heatmap_log_freq":5,
        
        
        "__comment1__": "It is possible to use winrate, random, latest, latest-set, highest, highest-set, lowest, lowest-set",
        "eval_metric": "~eval_metric",
        "eval_opponent_selection": "random",
        "opponent_selection": "random",
        "training_agent_selection": "latest",

        "num_sampled_opponent_per_round": "~num_sampled_opponent_per_round",
        "sample_after_rollout": "~sample_after_rollout",
        "sample_after_reset": "~sample_after_reset",
        "randomly_reseed_sampling": "~randomly_reseed_sampling",

        "rl_algorithm": "PPO",
        "policy": "MlpPolicy",
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "batch_size":64,
        "gamma":0.99,
        "lr": 3e-4,
        "n_epochs": 10
    },

    "agent1":{
        "id": 0,
        "name": "prey",
        "env_class": "SelfPlayPZPreyEnv",
        "opponent_name":"pred",
        "obs": "full",
        "act": "vel",


        "num_eval_episodes": 5,  
        "num_heatmap_eval_episodes": 5,  
        
        "num_timesteps": 25e3,

        "eval_freq": 0,
        "save_freq": 0,
        "final_save_freq": 5,

        "eval_matrix_testing_freq":3,

        "aggregate_eval_matrix":false,
        "heatmap_log_freq":5,
        
        
        "__comment1__": "It is possible to use winrate, random, latest, latest-set, highest, highest-set, lowest, lowest-set",
        "eval_metric": "~eval_metric",
        "eval_opponent_selection": "random",
        "opponent_selection": "random",
        "training_agent_selection": "latest",

        "num_sampled_opponent_per_round": "~num_sampled_opponent_per_round",
        "sample_after_rollout": "~sample_after_rollout",
        "sample_after_reset": "~sample_after_reset",
        "randomly_reseed_sampling": "~randomly_reseed_sampling",

        "rl_algorithm": "PPO",
        "policy": "MlpPolicy",
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "batch_size":64,
        "gamma":0.99,
        "lr": 3e-4,
        "n_epochs": 10
    }
}
